---
title: 透過範例瞭解 Azure Data Factory 定價
description: 此文章透過詳細範例說明及示範 Azure Data Factory 的定價模型
documentationcenter: ''
author: djpmsft
ms.author: daperlov
manager: jroth
ms.reviewer: maghan
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.date: 12/27/2019
ms.openlocfilehash: 9d96e3f7d127f4839592e766537cbdb07cc697dc
ms.sourcegitcommit: 877491bd46921c11dd478bd25fc718ceee2dcc08
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 07/02/2020
ms.locfileid: "81414930"
---
# <a name="understanding-data-factory-pricing-through-examples"></a>透過範例了解 Data Factory 定價

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

此文章透過詳細範例說明及示範 Azure Data Factory 的定價模型。

> [!NOTE]
> 下列範例中使用的價格為假設，並非表示實際定價。

## <a name="copy-data-from-aws-s3-to-azure-blob-storage-hourly"></a>每小時將資料從 AWS S3 複製到 Azure Blob 儲存體

在此案例中，您希望依每小時的排程將資料從 AWS S3 複製到 Azure Blob 儲存體。

若要完成案例，您需要使用下列項目建立管線：

1. 複製活動，包含要從 AWS S3 複製之資料的輸入資料集。

2. Azure 儲存體上之資料的輸出資料集。

3. 每小時執行管線的排程觸發程序。

   ![案例 1](media/pricing-concepts/scenario1.png)

| **作業** | **類型與單位** |
| --- | --- |
| 建立連結的服務 | 2 個讀取/寫入實體  |
| 建立資料集 | 4 個讀取/寫入實體 (2 個用於建立資料集，2 個用於連結的服務參考) |
| 建立管線 | 3 個讀取/寫入實體 (1 個用於建立管線，2 個用於資料集參考) |
| 取得管線 | 1 個讀取/寫入實體 |
| 執行管線 | 2 個活動執行 (1 個用於觸發程序執行，1 個用於活動執行) |
| 複製資料假設︰執行時間 = 10 分鐘 | 10 \* 4 Azure 整合執行階段 (預設 DIU 設定 = 4) 如需有關資料整合單位和最佳化複製效能的詳細資訊，請參閱[此文章](copy-activity-performance.md) |
| 監視管線假設：僅發生 1 次執行 | 2 個重試的監視執行記錄 (1 個用於管線執行，1 個用於活動執行) |

**總案例定價：$0.16811**

- Data Factory 作業 = **$0.0001**
  - 讀取/寫入 = 10\*00001 = $0.0001 [1 讀取/寫入 = $0.50/50000 = 0.00001]
  - 監視 = 2\*000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- 管線協調流程 &amp; 執行 = **$0.168**
  - 活動執行 = 001\*2 = 0.002 [1 執行 = $1/1000 = 0.001]
  - 資料移動活動 = $0.166 (依比例分配 10 分鐘的執行時間。 Azure 整合執行階段上每小時 0.25 美元)

## <a name="copy-data-and-transform-with-azure-databricks-hourly"></a>每小時複製資料，並使用 Azure Databricks 轉換

在此案例中，您希望依每小時的排程將資料從 AWS S3 複製到 Azure Blob 儲存體，並使用 Azure Databricks 轉換。

若要完成案例，您需要使用下列項目建立管線：

1. 一個複製活動，包含要從 AWS S3 複製資料的輸入資料集，以及 Azure 儲存體上之資料的輸出資料集。
2. 一個用於資料轉換的 Azure Databricks 活動。
3. 一個每小時執行管線的排程觸發程序。

![案例 2](media/pricing-concepts/scenario2.png)

| **作業** | **類型與單位** |
| --- | --- |
| 建立連結的服務 | 3 個讀取/寫入實體  |
| 建立資料集 | 4 個讀取/寫入實體 (2 個用於建立資料集，2 個用於連結的服務參考) |
| 建立管線 | 3 個讀取/寫入實體 (1 個用於建立管線，2 個用於資料集參考) |
| 取得管線 | 1 個讀取/寫入實體 |
| 執行管線 | 3 個活動執行 (1 個用於觸發程序執行，2 個用於活動執行) |
| 複製資料假設︰執行時間 = 10 分鐘 | 10 \* 4 Azure 整合執行階段 (預設 DIU 設定 = 4) 如需有關資料整合單位和最佳化複製效能的詳細資訊，請參閱[此文章](copy-activity-performance.md) |
| 監視管線假設：僅發生 1 次執行 | 3 個重試的監視執行記錄 (1 個用於管線執行，2 個用於活動執行) |
| 執行 Databricks 活動假設：執行時間 = 10 分鐘 | 10 分鐘外部管線活動執行 |

**總案例定價：$0.16916**

- Data Factory 作業 = **$0.00012**
  - 讀取/寫入 = 11\*00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 監視 = 3\*000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- 管線協調流程 &amp; 執行 = **$0.16904**
  - 活動執行 = 001\*3 = 0.003 [1 執行 = $1/1000 = 0.001]
  - 資料移動活動 = $0.166 (依比例分配 10 分鐘的執行時間。 Azure 整合執行階段上每小時 0.25 美元)
  - 外部管線活動 = $0.000041 (依比例分配 10 分鐘的執行時間。 Azure 整合執行階段上每小時 0.00025 美元)

## <a name="copy-data-and-transform-with-dynamic-parameters-hourly"></a>每小時複製資料並使用動態參數進行轉換

在此案例中，您希望依每小時的排程將資料從 AWS S3 複製到 Azure Blob 儲存體，並使用 Azure Databricks (透過指令碼中的動態參數) 轉換。

若要完成案例，您需要使用下列項目建立管線：

1. 一個複製活動，包含要從 AWS S3 複製資料的輸入資料集、Azure 儲存體上之資料的輸出資料集。
2. 一個 Lookup 活動，用於以動態方式將參數傳遞到轉換指令碼。
3. 一個用於資料轉換的 Azure Databricks 活動。
4. 一個每小時執行管線的排程觸發程序。

![案例 3](media/pricing-concepts/scenario3.png)

| **作業** | **類型與單位** |
| --- | --- |
| 建立連結的服務 | 3 個讀取/寫入實體  |
| 建立資料集 | 4 個讀取/寫入實體 (2 個用於建立資料集，2 個用於連結的服務參考) |
| 建立管線 | 3 個讀取/寫入實體 (1 個用於建立管線，2 個用於資料集參考) |
| 取得管線 | 1 個讀取/寫入實體 |
| 執行管線 | 4 個活動執行 (1 個用於觸發程序執行，3 個用於活動執行) |
| 複製資料假設︰執行時間 = 10 分鐘 | 10 \* 4 Azure 整合執行階段 (預設 DIU 設定 = 4) 如需有關資料整合單位和最佳化複製效能的詳細資訊，請參閱[此文章](copy-activity-performance.md) |
| 監視管線假設：僅發生 1 次執行 | 4 個重試的監視執行記錄 (1 個用於管線執行，3 個用於活動執行) |
| 執行 Lookup 活動假設：執行時間 = 1 分鐘 | 1 分鐘管線活動執行 |
| 執行 Databricks 活動假設：執行時間 = 10 分鐘 | 10 分鐘外部管線活動執行 |

**總案例定價：$0.17020**

- Data Factory 作業 = **$0.00013**
  - 讀取/寫入 = 11\*00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 監視 = 4\*000005 = $0.00002 [1 監視 = $0.25/50000 = 0.000005]
- 管線協調流程 &amp; 執行 = **$0.17007**
  - 活動執行 = 001\*4 = 0.004 [1 執行 = $1/1000 = 0.001]
  - 資料移動活動 = $0.166 (依比例分配 10 分鐘的執行時間。 Azure 整合執行階段上每小時 0.25 美元)
  - 管線活動 = $0.00003 (依比例分配 1 分鐘的執行時間。 Azure 整合執行階段上每小時 $0.002 美元)
  - 外部管線活動 = $0.000041 (依比例分配 10 分鐘的執行時間。 Azure 整合執行階段上每小時 0.00025 美元)

## <a name="using-mapping-data-flow-debug-for-a-normal-workday"></a>針對一般 workday 使用對應資料流程 debug

身為數據工程師，您必須負責每天設計、建立和測試對應的資料流程。 您會在早上登入 ADF UI，並啟用資料流程的「偵錯工具」模式。 Debug 會話的預設 TTL 為60分鐘。 您每天的工作時間為8小時，因此您的 Debug 會話永不過期。 因此，您當天的費用將會是：

**8（小時） x 8 （計算優化核心） x $0.193 = $12.35**

## <a name="transform-data-in-blob-store-with-mapping-data-flows"></a>使用對應資料流程轉換 blob 存放區中的資料

在此案例中，您想要以每小時的排程，以視覺化方式在 ADF 對應資料流程中轉換 Blob 存放區中的資料。

若要完成案例，您需要使用下列項目建立管線：

1. 具有轉換邏輯的資料流程活動。

2. Azure 儲存體上資料的輸入資料集。

3. Azure 儲存體上之資料的輸出資料集。

4. 每小時執行管線的排程觸發程序。

| **作業** | **類型與單位** |
| --- | --- |
| 建立連結的服務 | 2 個讀取/寫入實體  |
| 建立資料集 | 4 個讀取/寫入實體 (2 個用於建立資料集，2 個用於連結的服務參考) |
| 建立管線 | 3 個讀取/寫入實體 (1 個用於建立管線，2 個用於資料集參考) |
| 取得管線 | 1 個讀取/寫入實體 |
| 執行管線 | 2 個活動執行 (1 個用於觸發程序執行，1 個用於活動執行) |
| 資料流程假設：執行時間 = 10 分鐘 + 10 最小 TTL | 一般計算的10個 \* 16 核心，TTL 為10 |
| 監視管線假設：僅發生 1 次執行 | 2 個重試的監視執行記錄 (1 個用於管線執行，1 個用於活動執行) |

**總案例定價： $1.4631**

- Data Factory 作業 = **$0.0001**
  - 讀取/寫入 = 10\*00001 = $0.0001 [1 讀取/寫入 = $0.50/50000 = 0.00001]
  - 監視 = 2\*000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- 管線協調流程 &amp; 執行 = **$1.463**
  - 活動執行 = 001\*2 = 0.002 [1 執行 = $1/1000 = 0.001]
  - 資料流程活動 = $1.461 已按比例計費20分鐘（執行時間為10分鐘，TTL 為10分鐘）。 Azure Integration Runtime 上的 $ 0.274/小時具有16個核心的一般計算

## <a name="next-steps"></a>後續步驟

既然您了解 Azure Data Factory 的定價，您可以立即開始！

- [使用 Azure Data Factory UI 來建立資料處理站](quickstart-create-data-factory-portal.md)

- [Azure Data Factory 簡介](introduction.md)

- [Azure Data Factory 中的視覺化撰寫](author-visually.md)
