---
title: 潛在的狄氏配置：模組參考
titleSuffix: Azure Machine Learning
description: 瞭解如何使用潛在的狄氏配置模組，將非機密文字分組為類別。
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: likebupt
ms.author: keli19
ms.date: 06/05/2020
ms.openlocfilehash: f9f239ea69aaf71e591a447feb300c13a45ba1a4
ms.sourcegitcommit: 829d951d5c90442a38012daaf77e86046018e5b9
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 10/09/2020
ms.locfileid: "90907861"
---
# <a name="latent-dirichlet-allocation-module"></a>潛在的狄氏配置模組

本文說明如何使用 Azure Machine Learning 設計工具中的潛在狄氏配置模組，將非機密文字分組為類別。 

潛在的狄氏配置 (LDA) 通常用於自然語言處理，以尋找類似的文字。 另一個常見的詞彙是 *主題模型*化。

此課程模組會使用一個文字資料行，並產生下列輸出：

+ 來源文字，以及每個類別的分數

+ 包含每個類別之已解壓縮詞彙和係數的 feature 矩陣

+ 轉換，您可以儲存並重新套用至做為輸入使用的新文字

本課程模組使用 scikit-learn 學習程式庫。 如需 scikit-learn-學習的詳細資訊，請參閱 [GitHub 存放庫](https://github.com/scikit-learn/scikit-learn)，其中包含教學課程和演算法的說明。

## <a name="more-about-latent-dirichlet-allocation"></a>深入瞭解潛在的狄氏配置

LDA 通常不是分類的方法。 但是，它會使用有生產力方法，因此您不需要提供已知的類別標籤，然後推斷模式。  相反地，此演算法會產生用來識別主題群組的概率模型。 您可以使用概率模型，將現有的定型案例或您提供給模型的新案例分類為輸入。

您可能會偏好使用有生產力模型，因為它可避免對文字和分類之間的關聯性做出強烈的假設。 它只會使用單字的分佈，以數學方式建立主題的模型。

本檔將討論這項理論，可從 PDF 下載： [潛在的狄氏配置： Blei、Ng 和約旦](https://ai.stanford.edu/~ang/papers/nips01-lda.pdf)。

此課程模組中的實作為以適用于 LDA 的 [scikit-learn 學習程式庫](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/_lda.py) 為基礎。

如需詳細資訊，請參閱 [技術附注](#technical-notes) 一節。

## <a name="how-to-configure-latent-dirichlet-allocation"></a>如何設定潛在的狄氏配置

此模組需要包含文字資料行的資料集（未經處理或預先處理）。

1. 將 **潛在的狄氏配置** 模組新增至您的管線。

2. 作為模組的輸入，提供包含一或多個文字資料行的資料集。

3. 針對 [ **目標資料行**]，選擇包含要分析之文字的一或多個資料行。

    您可以選擇多個資料行，但這些資料行必須是 **字串** 資料類型。

    因為 LDA 會從文字建立大型的特徵矩陣，所以您通常會分析單一文字資料行。

4. **若要建立模型的主題數目**，請輸入介於1到1000之間的整數，指出您想要從輸入文字中衍生的類別或主題數目。

    預設會建立5個主題。

5. 若為 **N**字母，請指定在雜湊期間產生的 n 字母長度上限。

    預設值為2，表示會產生雙字母組和 unigrams。

6. 選取 [ **標準化** ] 選項，將輸出值轉換為機率。 

    輸出和特徵資料集中的值，不會以整數表示轉換的值，而是轉換如下：

    + 資料集中的值會以機率表示 `P(topic|document)` 。

    + 功能主題矩陣中的值會以機率表示 `P(word|topic)` 。

    > [!NOTE] 
    > 在 Azure Machine Learning 設計工具中，scikit-learn 學習程式庫不再支援0.19 版的非標準化 *doc_topic_distr* 輸出。 在此課程模組中， **標準化** 參數只能套用至 *功能主題矩陣* 輸出。 *轉換的資料集* 輸出一律會正規化。

7. 選取 [ **顯示所有選項**] 選項，如果您想要設定下列 advanced 參數，請將它設定為 [ **TRUE** ]。

    這些參數專屬於 LDA 的 scikit-learn 學習執行。 在 scikit-learn 中 LDA 有一些不錯的教學課程，以及官方的 [scikit-learn 學習檔](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)。

    + **Rho 參數**。 提供主題分佈之稀疏性的先前機率。 此參數對應于 sklearn 的 `topic_word_prior` 參數。 如果您預期單字的分佈是平面的，請使用值 **1** 。也就是說，所有字組都是假設為 equiprobable。 如果您認為大部分的單字是稀疏的，您可能會將它設定為較低的值。

    + **Alpha 參數**。 針對每個檔主題權數的稀疏性指定先前的機率。 此參數對應于 sklearn 的 `doc_topic_prior` 參數。

    + **估計的檔數目**。 輸入數位，代表將處理之檔 (資料) 列數目的最佳預估。 此參數可讓模組配置足夠大小的雜湊表。 它會對應到 `total_samples` scikit-learn 中的參數（學習）。

    + **批次的大小**。 輸入數位，表示傳送至 LDA 模型的每個文字批次中要包含多少資料列。 此參數會對應至 `batch_size` scikit-learn 中的參數（學習）。

    + **學習更新排程中所使用之反復專案的初始值**。 指定可在線上學習中 downweights 早期反覆運算學習率的起始值。 此參數會對應至 `learning_offset` scikit-learn 中的參數（學習）。

    + **在更新期間套用至反覆運算的電源**。 指出套用至反覆運算計數的電源等級，以控制線上更新期間的學習速率。 此參數會對應至 `learning_decay` scikit-learn 中的參數（學習）。

    + **資料的傳遞數目**。 指定演算法將在資料上迴圈的最大次數。 此參數會對應至 `max_iter` scikit-learn 中的參數（學習）。

8. 如果您想要在分類文字之前，先在初始階段中建立 n 語法的清單，請選取 [LDA 之前的 **ngrams** 或 **ngrams 的組建字典**] 選項。

    如果您事先建立初始字典，您稍後可以在查看模型時使用字典。 能夠將結果對應至文字，而不是數值索引，通常比較容易轉譯。 不過，儲存字典需要較長的時間，並使用額外的儲存體。

9. 在 [ **ngram 字典的大小上限**] 中，輸入可在 n 語法字典中建立的資料列總數。

    此選項適用于控制字典的大小。 但是，如果輸入中的 ngrams 數目超過此大小，則可能會發生衝突。

10. 提交管線。 LDA 模組會使用貝氏機率分類定理來判斷哪些主題可能與個別單字相關聯。 單字並非專門與任何主題或群組相關聯。 相反地，每個 n 元都有一個與任何探索到的類別相關聯的學習可能性。

## <a name="results"></a>結果

此模組有兩個輸出：

+ 已**轉換的資料集**：此輸出包含輸入文字、指定數目的已探索類別，以及每個類別的每個文字範例分數。

+ **功能主題矩陣**：最左邊的資料行包含已解壓縮的文字功能。 每個分類的資料行都包含該分類中該功能的分數。


### <a name="lda-transformation"></a>LDA 轉換

此模組也會輸出將 LDA 套用至資料集的 *LDA 轉換* 。

您可以儲存此轉換，並將其重複用於其他資料集。 如果您已經針對大型主體進行定型，而且想要重複使用係數或類別，這項技術可能會很有用。

若要重複使用此轉換，請選取潛在狄氏配置模組右面板中的 [ **註冊資料集** ] 圖示，以將模組保留在模組清單中的 [ **資料集** ] 類別之下。 然後，您可以將此模組連接至套用 [轉換](apply-transformation.md) 模組，以重複使用此轉換。

### <a name="refining-an-lda-model-or-results"></a>精簡 LDA 模型或結果

一般來說，您無法建立可符合所有需求的單一 LDA 模型。 即使是針對一個工作所設計的模型，也可能需要許多反復專案來改善精確度。 建議您嘗試所有這些方法來改善您的模型：

+ 變更模型參數
+ 使用視覺效果來瞭解結果
+ 取得主題專家的意見反應，以判斷產生的主題是否有用

定性量值也有助於評估結果。 若要評估主題模型化結果，請考慮：

+ 精確度。 類似專案真的很類似嗎？
+ 多樣性。 當商務問題需要時，模型是否可以區分類似專案？
+ 延展性。 它是否適用于各式各樣的文字類別，或僅適用于較窄的目標網域？

您通常可以使用自然語言處理來清除、摘要和簡化或分類文字，以改善以 LDA 為基礎的模型精確度。 例如，下列技術（Azure Machine Learning 中全部支援）可以改善分類的精確度：

+ 停用字詞移除

+ 案例正規化

+ 詞形歸併還原或詞幹分析

+ 具名實體辨識

如需詳細資訊，請參閱前置處理 [文字](preprocess-text.md)。

在設計工具中，您也可以使用 R 或 Python 程式庫進行文字處理： [執行 r 腳本](execute-r-script.md)、  [執行 Python 腳本](execute-python-script.md)。



## <a name="technical-notes"></a>技術說明

本節包含對常見問題的執行詳細資料、秘訣和解答。

### <a name="implementation-details"></a>實作詳細資料

根據預設，已轉換之資料集和功能主題矩陣的輸出分佈會正規化為機率：

+ 已轉換的資料集會正規化為主題指定檔的條件機率。 在此情況下，每個資料列的總和等於1。

+ 功能主題矩陣會正規化為指定主題之文字的條件機率。 在此情況下，每個資料行的總和等於1。

> [!TIP]
> 有時模組可能會傳回空白主題。 最常見的原因是演算法的虛擬隨機初始化。 如果發生這種情況，您可以嘗試變更相關的參數。 例如，變更 N 語法字典的大小上限，或變更特徵雜湊所使用的位數。

### <a name="lda-and-topic-modeling"></a>LDA 和主題模型化

潛在的狄氏配置通常用於以 *內容為基礎的主題模型*，基本上是指從非機密文字學習類別目錄。 在以內容為基礎的主題模型中，主題是透過單字的散發。

例如，假設您已提供客戶評論的主體，其中包含許多產品。 客戶在一段時間內提交的評論文字包含許多條款，其中有些是用於多個主題。

LDA 流程識別的 *主題* 可能代表個別產品的評論，或可能代表一組產品評論。 在 LDA 的情況下，主題本身就是一組字組一段時間的機率分佈。

條款對任何一項產品來說都很少。 這些產品可以參考其他產品，也可以是適用于所有專案的一般詞彙 ( 「絕佳」、「操作」 ) 。 其他詞彙可能是非搜尋字。 但是，LDA 方法不會嘗試抓住宇宙中的所有字組，或瞭解單字如何相關，除了並存出現的機率之外。 它只能群組目標網域中使用的單字。

計算詞彙索引之後，以距離為基礎的相似性量值會比較個別的文字資料列，以判斷兩段文字是否類似。 例如，您可能會發現產品有多個強關聯的名稱。 或者，您可能會發現強烈的負詞彙通常會與特定產品相關聯。 您可以使用相似性量值來識別相關的字詞，以及建立建議。

###  <a name="module-parameters"></a>模組參數

|名稱|類型|範圍|選用|預設|描述|  
|----------|----------|-----------|--------------|-------------|-----------------|  
|目標資料行|資料行選取||必要|StringFeature|目標資料行名稱或索引。|  
|要建立模型的主題數目|整數|[1; 1000]|必要|5|針對 N 個主題建立檔散發的模型。|  
|N 字母組|整數|[1; 10]|必要|2|雜湊期間產生的 N 字母順序。|  
|規範|布林值|True 或 False|必要|true|將輸出標準化為機率。  已轉換的資料集將會是第一個&#124;檔) 的 (主題，而功能主題矩陣將會是 P (word&#124;主題) 。|  
|顯示所有選項|布林值|True 或 False|必要|False|提供 scikit-learn 專用的其他參數-學習線上 LDA。|  
|Rho 參數|Float|[0.00001; 1.0]|適用于選取 [ **顯示所有選項** ] 核取方塊時|0.01|先前發佈的主題文字。|  
|Alpha 參數|Float|[0.00001; 1.0]|適用于選取 [ **顯示所有選項** ] 核取方塊時|0.01|檔主題先前的散發。|  
|估計的檔數目|整數|[1;int.MaxValue]|適用于選取 [ **顯示所有選項** ] 核取方塊時|1000|估計的檔數目。 對應于 `total_samples` 參數。|  
|批次的大小|整數|[1; 1024]|適用于選取 [ **顯示所有選項** ] 核取方塊時|32|批次的大小。|  
|用於學習速率更新排程的反復專案初始值|整數|[0; int。Timespan.maxvalue|適用于選取 [ **顯示所有選項** ] 核取方塊時|0|Downweights 早期反覆運算學習速率的初始值。 對應于 `learning_offset` 參數。|  
|更新期間套用至反復專案的電源|Float|[0.0; 1.0]|適用于選取 [ **顯示所有選項** ] 核取方塊時|0.5|套用至反覆運算計數的電源，以控制學習速率。 對應于 `learning_decay` 參數。 |  
|反覆定型的次數|整數|[1; 1024]|適用于選取 [ **顯示所有選項** ] 核取方塊時|25|定型反覆運算的數目。|  
|Ngrams 的組建字典|布林值|True 或 False|適用于*未*選取 [**顯示所有選項**] 核取方塊時|True|在計算 LDA 之前，先建立 ngrams 的字典。 適用于模型檢查和轉譯。|  
|Ngram 字典的大小上限|整數|[1;int.MaxValue]|適用于 ngrams 的選項**建立字典**為**True**時|20000|Ngrams 字典的大小上限。 如果輸入中的權杖數目超過此大小，可能會發生衝突。|  
|要用於特徵雜湊的位元組數目。|整數|[1; 31]|適用于*未*選取 [**顯示所有選項**] 核取方塊，且**Ngrams 的組建字典**為**False**時|12|要用於特徵雜湊的位元組數目。| 
|LDA 之前的 ngrams 建立字典|布林值|True 或 False|適用于選取 [ **顯示所有選項** ] 核取方塊時|True|在 LDA 之前建立 ngrams 的字典。 適用于模型檢查和轉譯。|  
|字典中的 ngrams 數目上限|整數|[1;int.MaxValue]|適用于選取 [**顯示所有選項**] 核取方塊，且 [ngrams] 的選項**組建字典**為**True**時|20000|字典的大小上限。 如果輸入中的權杖數目超過此大小，可能會發生衝突。|  
|雜湊位數目|整數|[1; 31]|適用于選取 [**顯示所有選項**] 核取方塊，且 [ngrams] 的 [**建立字典**] 選項為**False**時|12|在特徵雜湊期間要使用的位元組數目。|   


## <a name="next-steps"></a>後續步驟

請參閱 Azure Machine Learning 的[可用模組集](module-reference.md)。 

如需模組特定的錯誤清單，請參閱設計工具的 [例外狀況和錯誤碼](designer-error-codes.md)。
