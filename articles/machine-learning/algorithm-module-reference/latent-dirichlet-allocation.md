---
title: 潛移默化分配
titleSuffix: Azure Machine Learning
description: 瞭解如何使用"潛伏迪裡切分配"模組將其他未分類的文本分組到多個類別中。
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: likebupt
ms.author: keli19
ms.date: 03/11/2020
ms.openlocfilehash: 1384491489c175ffc338f80a99aa8d5050f835d5
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 03/28/2020
ms.locfileid: "80109221"
---
# <a name="latent-dirichlet-allocation"></a>潛移默化分配

本文介紹如何使用 Azure 機器學習設計器（預覽）中的 **"潛在迪裡切分配"** 模組將其他未分類的文本分組到多個類別中。 

潛移默分配 （LDA） 通常用於自然語言處理 （NLP） 來查找類似的文本。 另一個常用術語是*主題建模*。

此模組採用一列文本，並生成以下輸出：

+ 源文本，以及每個類別的分數

+ 包含每個類別提取的術語和係數的特徵矩陣

+ 轉換，您可以保存並重新應用於用作輸入的新文本

本模組使用學子庫。 有關 scikit 學習的詳細資訊，請參閱 #GitHub 存儲庫，其中包含教程和演算法說明。

### <a name="more-about-latent-dirichlet-allocation-lda"></a>更多關於潛伏性迪裡切分配 （LDA）

一般來說，LDA本身並不是一種分類方法，而是使用生成方法。 這意味著您不需要提供已知的類標籤，然後推斷模式。  相反，該演算法生成一個概率模型，用於標識主題組。 可以使用概率模型對現有訓練案例或作為輸入提供給模型的新案例進行分類。

生成模型可能更可取，因為它避免對文本和類別之間的關係做出任何強烈的假設，並且僅將單詞的分佈用於數學模型主題。

+ 本文討論了這一理論，可作為 PDF 下載：[潛伏的迪裡切分配：布萊、Ng 和約旦](https://ai.stanford.edu/~ang/papers/nips01-lda.pdf)

+ 本模組中的實現基於 LDA[的 scikit 學習庫](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/_lda.py)。

有關詳細資訊，請參閱[技術說明](#technical-notes)部分。

## <a name="how-to-configure-latent-dirichlet-allocation"></a>如何配置延遲迪裡奇特分配

此模組需要包含文本列（原始或預處理）的資料集。

1. 將 **"潛伏的迪裡切分配**"模組添加到管道中。

2. 作為模組的輸入，提供包含一個或多個文本列的資料集。

3. 對於**目標列**，選擇要分析的一個或多個包含文本的列。

    您可以選擇多個列，但它們必須是字串資料類型的列。

    通常，由於 LDA 從文本創建大型要素矩陣，因此通常會分析單個文本列。

4. 對於**要建模的主題數**，鍵入介於 1 和 1000 之間的整數，指示要從輸入文本派生的類別或主題數。

    預設情況下，將創建 5 個主題。

5. 對於**N-gram，** 指定雜湊期間生成的 N-gram 的最大長度。

    預設值為 2，這意味著生成大拉姆和單格。

6. 選擇 **"正常化**"選項以將輸出值轉換為概率。 因此，輸出和要素資料集中的值將轉換如下：而不是將轉換後的值表示為整數：

    + 資料集中的值將表示為 概率。 `P(topic|document)`

    + 要素主題矩陣中的值將表示為`P(word|topic)`概率。

    > [!NOTE] 
    > 在 Azure 機器學習設計器（預覽）中，由於我們基於的庫學 scikit 學習，不再支援版本 0.19 中的非正常化*doc_topic_distr*輸出，因此，在此模組中，**正常化**參數只能應用於**要素主題矩陣**輸出，**轉換資料集**輸出始終正常化。

7. 選擇選項"**顯示所有選項**"，然後將其設置為 TRUE（如果要查看，然後設置其他高級參數）。

    這些參數特定于 LDA 的 scikit 學習實現。 有一些關於LDA的輔導在scikit學習，以及官方的[scikit學習檔](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)。

    + **Rho 參數**. 提供主題分佈稀疏性的先前概率。 對應于 sklearn 的`topic_word_prior`參數。 如果預期單詞的分佈是平的，則可以使用值 1;也就是說，所有單詞都是假定相等的。 如果您認為大多數單詞以稀疏方式顯示，則可以將其設置為低得多的值。

    + **阿爾法參數**. 為每個文檔主題權重的稀疏性指定先前的概率。  對應于 sklearn 的`doc_topic_prior`參數。

    + **估計檔數量**。 鍵入一個數位，表示要處理的文檔（行）數的最佳估計值。 這允許模組分配足夠大大小的雜湊表。  對應于學中的`total_samples`參數。

    + **批次處理的大小**。 鍵入一個數位，指示發送到 LDA 模型的每批文本中要包含多少行。 對應于學中的`batch_size`參數。

    + **學習更新計畫中使用的反覆運算的初始值**。 指定降低線上學習早期反覆運算學習速率的起始值。 對應于學中的`learning_offset`參數。

    + **在更新期間應用於反覆運算的電源**。 指示應用於反覆運算計數的功率級別，以便在連線更新期間控制學習速率。 對應于學中的`learning_decay`參數。

    + **通過資料的次數**。 指定演算法在資料中迴圈的最大次數。 對應于學中的`max_iter`參數。

8. 如果要在初始傳遞中創建 n-gram 清單，請在對文本進行分類之前選擇"**生成 ngram 字典**"或 **"在 LDA 之前生成 ngram 詞典**"選項。

    如果事先創建初始字典，則稍後可以在查看模型時使用字典。 能夠將結果映射到文本而不是數位索引通常更容易解釋。 但是，保存字典將需要更長的時間，並使用額外的存儲。

9. 對於**ngram 字典的最大大小**，鍵入可在 n-gram 字典中創建的行總數。

    此選項可用於控制字典的大小。 但是，如果輸入中的 ngram 數超過此大小，則可能發生衝突。

10. 提交管道。 LDA 模組使用貝葉斯定理來確定哪些主題可能與單個單詞相關聯。 單詞不只與任何主題或組相關聯;相反，每個 n-gram 都有與任何發現的類關聯的學習概率。

## <a name="results"></a>結果

此模組有兩個輸出：

+ **轉換後的資料集**：包含輸入文本和指定數量的已發現類別，以及每個類別的每個文本示例的分數。

+ **要素主題矩陣**：最左側的列包含提取的文本要素，並且每個類別都有一列，其中包含該類別中該要素的分數。


### <a name="lda-transformation"></a>LDA 轉換

此模組還輸出將 LDA 應用於資料集的*LDA 轉換*。

您可以通過在模組右側窗格中的 **"輸出_logs"** 選項卡下註冊資料集來保存此轉換，並將其重新用於其他資料集。 如果您已對大型語料庫進行了培訓，並希望重用係數或類別，這可能很有用。

### <a name="refining-an-lda-model-or-results"></a>優化 LDA 模型或結果

通常，您不能創建滿足所有需求的單個 LDA 模型，即使為一個任務設計的模型也可能需要多次反覆運算來提高準確性。 我們建議您嘗試所有這些方法來改進模型：

+ 更改模型參數
+ 使用視覺化來瞭解結果
+ 獲取主題專家的回饋，以確定生成的主題是否有用。

定性措施也可用於評估結果。 要評估主題建模結果，請考慮：

+ 準確性 - 類似專案真的相似嗎？
+ 多樣性 - 當業務問題需要時，模型是否可以區分類似的專案？
+ 可伸縮性 - 它是在廣泛的文本類別上工作，還是僅在狹窄的目標域上工作？

通過使用自然語言處理來清理、總結和簡化或分類文本，通常可以提高基於 LDA 的模型的準確性。 例如，Azure 機器學習中都支援以下技術可以提高分類準確性：

+ 停用字詞移除

+ 案例正常化

+ lemmat 化或詞幹

+ 具名實體辨識

有關詳細資訊，請參閱[預處理文本](preprocess-text.md)。

在設計器中，還可以使用 R 或 Python 庫進行文本處理：[執行 R 腳本](execute-r-script.md)、執行 Python[腳本](execute-python-script.md)



## <a name="technical-notes"></a>技術說明

本節包含實現詳細資訊、提示和對常見問題的解答。

### <a name="implementation-details"></a>實作詳細資料

預設情況下，轉換資料集和要素主題矩陣的輸出分佈將歸化為概率。

+ 轉換後的資料集正常化為給定文檔的主題的條件概率。 在這種情況下，每行的總和等於 1。

+ 特徵主題矩陣被正常化為給定主題的單詞的條件概率。 在這種情況下，每列的總和等於 1。

> [!TIP]
> 有時，模組可能會返回一個空主題，這通常是由演算法的偽隨機初始化引起的。  如果發生這種情況，您可以嘗試更改相關參數，例如 N-gram 字典的最大大小或用於特徵雜湊的位數。

### <a name="lda-and-topic-modeling"></a>LDA 和主題建模

潛移默分配 （LDA） 通常用於*基於內容的主題建模*，這基本上意味著從非機密文本學習類別。 在基於內容的主題建模中，主題是單詞的分發。

例如，假設您提供了包含許多產品的客戶評論。 許多客戶隨著時間的推移提交的評審文本將包含許多術語，其中一些術語用於多個主題。

LDA 流程標識**的主題**可能表示單個產品 A 的審核，或者表示一組產品審核。 對於 LDA，主題本身只是一組單詞隨時間分佈的概率。

術語很少是任何一種產品獨有的，但可以指其他產品，也可以指適用于所有產品的一般術語（"偉大"、"糟糕"）。 其他術語可能是噪音詞。  然而，重要的是要理解，LDA方法並不意味著捕獲宇宙中的所有單詞，或理解單詞是如何相關的，除了共發生的概率。 它只能對目標域中使用的單詞進行分組。

計算術語索引後，使用基於距離的相似性度量對單個行的文本進行比較，以確定兩段文本是否彼此相似。  例如，您可能會發現產品有多個名稱具有高度相關。 或者，您可能會發現，強烈的負面術語通常與特定產品相關聯。 您可以使用相似性度量來標識相關術語並創建建議。

###  <a name="module-parameters"></a>模組參數

|名稱|類型|範圍|選用|預設|描述|  
|----------|----------|-----------|--------------|-------------|-----------------|  
|目標資料行|資料行選取||必要|StringFeature|目標列名稱或索引|  
|要建模的主題數|整數 |[1;1000]|必要|5|根據 N 個主題對文檔分發進行建模|  
|N 字母組|整數 |[1;10]|必要|2|雜湊期間生成的 N 克順序|  
|規範|Boolean|True 或 False|必要|true|將輸出正常化為概率。  轉換後的資料集為 P（主題&#124;文檔），要素主題矩陣為 P（單詞&#124;主題）|  
|顯示所有選項|Boolean|True 或 False|必要|False|提供特定于 scikit 學習線上 LDA 的其他參數|  
|Rho 參數|Float|[0.00001;1.0]|選中"**顯示所有選項**"核取方塊時應用|0.01|主題字事先分發|  
|阿爾法參數|Float|[0.00001;1.0]|選中"**顯示所有選項**"核取方塊時應用|0.01|文檔主題提前分發|  
|估計檔數量|整數 |[1;int.MaxValue]|選中"**顯示所有選項**"核取方塊時應用|1000|估計的文檔數（對應于total_samples參數）|  
|批次的大小|整數 |[1;1024]|選中"**顯示所有選項**"核取方塊時應用|32|批次的大小|  
|學習速率更新計畫中使用的反覆運算的初始值|整數 |{0;int.最大價值*|選中"**顯示所有選項**"核取方塊時應用|0|早期反覆運算降低學習速率的初始值。 對應于learning_offset參數|  
|在更新期間應用於反覆運算的電源|Float|[0.0;1.0]|選中"**顯示所有選項**"核取方塊時應用|0.5|應用於反覆運算計數以控制學習速率的功率。 對應于learning_decay參數 |  
|反覆定型的次數|整數 |[1;1024]|選中"**顯示所有選項**"核取方塊時應用|25|反覆定型的次數|  
|生成 ngram 詞典|Boolean|True 或 False|未選擇"**顯示所有選項**"核取方塊時*not*應用|True|在計算 LDA 之前構建 ngram 詞典。 可用於模型檢查和解釋|  
|ngram 字典的最大大小|整數 |[1;int.MaxValue]|當**選項"生成 ngram"字典**為 true 時應用|20000|ngram 字典的最大大小。 如果輸入中的權杖數超過此大小，則可能發生衝突|  
|用於特徵雜湊的位數|整數 |[1;31]|當*未*選擇 **"顯示所有選項**"核取方塊，並且**ngram 的生成字典**為 False 時應用|12|用於特徵雜湊的位數| 
|在 LDA 之前構建 ngram 詞典|Boolean|True 或 False|選中"**顯示所有選項**"核取方塊時應用|True|在 LDA 之前構建 ngram 詞典。 可用於模型檢查和解釋|  
|字典中的最大 ngram 數|整數 |[1;int.MaxValue]|當選中"**顯示所有選項**"核取方塊，並且 **"生成 ngram"詞典**的選項為 True 時應用|20000|字典的最大大小。 如果輸入中的權杖數超過此大小，則可能發生衝突|  
|雜湊位數|整數 |[1;31]|當選中"**顯示所有選項**"核取方塊，並且 **"生成 ngram"選項**為 False 時應用|12|在特徵雜湊期間要使用的位數|   


## <a name="next-steps"></a>後續步驟

請參閱 Azure 機器學習[可用的模組集](module-reference.md)。   
有關特定于模組的錯誤清單，請參閱[設計器的異常和錯誤代碼](designer-error-codes.md)。
