---
title: 道德和負責任的使用 - 個人化工具
titleSuffix: Azure Cognitive Services
description: 這些指導方針旨在協助您以有助於在公司和服務中建立信任的方式實作個人化。 一定要停下來研究,學習和思考個人化對人們生活的影響。 如有疑問，請尋求指引。
services: cognitive-services
author: diberry
manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 06/12/2019
ms.author: diberry
ms.openlocfilehash: e422284b871214dbeca31b5dd17b9177a18ad3c8
ms.sourcegitcommit: efefce53f1b75e5d90e27d3fd3719e146983a780
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 04/01/2020
ms.locfileid: "80478102"
---
# <a name="guidelines-for-responsible-implementation-of-personalizer"></a>負責任地實作個人化工具的指導方針

為了讓社會大眾了解充分發揮 AI 的潛能，設計實作時應採適當方式，讓透過 AI 建置的應用程式以及在其應用程式中新增 AI 的使用者能夠信任這些實作。 這些指導方針旨在協助您以有助於在公司和服務中建立信任的方式，來實作個人化工具。 一定要停下來研究,學習和思考個人化對人們生活的影響。 如有疑問，請尋求指引。

這些指導方針不適合作為法律建議，且您應自行確認您的應用程式符合此領域和您所屬部門中快速變動的法規制定。

此外，在設計使用個人化工具的應用程式時，您應多方面考量您在開發任何以資料為中心的 AI 系統時所需承擔的責任，包括道德、隱私權、資安、安全性、內容、透明度和責任等方面。 您可以在[建議閱讀資料](#recommended-reading)一節中深入了解上述事項。

您可以使用下列內容作為入門檢查清單，並根據您的案例加以自訂及精簡。 本文檔包含兩個主要部分:第一部分專門用於在選擇個人化程式的方案、功能和獎勵時突出顯示負責任的使用注意事項。 第二個部分將提供 Microsoft 認為在建置 AI 系統時所應考量的若干價值，並提供可採行的建議，以及評估您使用個人化工具時對系統產生影響的風險。


## <a name="your-responsibility"></a>您的責任

所有關於善盡實作責任的指導方針在建置時均採行下列基本原則：開發人員和使用個人化工具的企業都必須對在社會中使用這些演算法的效果負責。 如果您要開發組織所將部署的應用程式，您應了解自身的角色，以及您對於該應用程式的運作及其對社會大眾之影響的責任。 如果您要設計由第三方部署的應用程式，則應就應用程式行為的最終責任歸屬於哪一方，與第三方達成共識，並將其記載於文件。

履行的承諾是建立信任的基礎概念 - 請考量您的應用程式運作所在的使用者、社會和法律架構，以識別該架構可能會有的明確和隱含承諾。

Microsoft 會持續將心力投注於其工具和文件，以協助您善盡相關責任。 如果您認為有其他工具、產品功能和文件可協助您遵循這些使用個人化工具的指導方針，請[提供意見反應給 Microsoft](mailto:cogsvcs-RL-feedback@microsoft.com?subject%3DPersonalizer%20Responsible%20Use%20Feedback&body%3D%5BPlease%20share%20any%20question%2C%20idea%20or%20concern%5D)。


## <a name="factors-for-responsibly-implementing-personalizer"></a>善盡個人化工具實作責任的要素

實作個人化工具對於您的使用者和企業可能有極大的價值。 若要有責任地實作個人化工具，首先必須在下列情況下考量相關指導方針：

* 選擇要套用個人化工具的使用案例。
* 建置[回報函式](concept-rewards.md)。
* 選擇您要將哪些內容相關[功能](concepts-features.md)和可能的動作用於個人化。


## <a name="choosing-use-cases-for-personalizer"></a>選擇個人化工具的使用案例

使用可經由學習將內容和使用者介面個人化的服務，確實很有幫助。 但若個人化的方式會在實際環境中產生不良副作用 (包括使用者不知道內容會個人化)，也可能會有誤用的情形。

較有可能出現不良副作用或缺乏透明度的個人化工具使用案例，包括「回報」取決於許多長時間的複雜因素，而在過度簡化為立即的回報時可能對個人產生不利結果的案例。 這些往往被視為"間接"選擇,或涉及傷害風險的選擇。 例如：


* **金融**:貸款、金融和保險產品的個人化優惠,其中風險因素基於個人不知道、無法獲得或無法爭議的數據。
* **教育**:學校課程和教育機構的個人化排名,建議可能會傳播偏見,降低使用者對其他選擇的認識。
* **民主與公民參與**:以影響意見為目標的用戶個人化內容是間接的和操縱性的。
* **第三方獎勵評估**:根據後三方對用戶的評價,而不是由使用者自己的行為產生獎勵的個人化專案。
* **對探索的不寬容**:個人化者勘探行為可能造成傷害的任何情況。

選擇個人化工具的使用案例時：

* 展開設計程序，考量如何透過個人化協助使用者。
* 考量因個人化模式或探索而未向使用者提供某些項目的排名時，在實際環境中會產生何種負面後果。
* 考慮您的用例是否構成自動處理,這嚴重影響受[GDPR](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016R0679)第22條或其他法律規範的數據主體的影響。
* 考量自我兌現預言迴圈。 如果個人化回報訓練了某個模型，而使其後續可能會進一步排除某個人口族群對相關內容的存取，就可能會發生此情形。 例如,低收入社區的大多數人沒有獲得保費保險,而且,如果沒有足夠的探索,附近幾乎沒有人看到這種優惠。
* 儲存模型和學習原則的複本，以備日後需要重現個人化工具時使用。 您可以定期或在每個模型重新整理期間執行此作業。
* 考量空間所需的探索層級，以及如何利用它來降低「同溫層」效應。


## <a name="selecting-features-for-personalizer"></a>選取個人化工具的功能

要將內容個人化，必須要有關於內容和使用者的實用資訊。 請注意，就某些應用程式和產業而言，某些特性可能會被直接或間接視為歧視性或非法特性。

請考量下列特性的效應：

* **使用者人口統計**:有關性別、性別、年齡、種族、宗教的特徵:出於監管原因,某些應用程式中可能不允許使用這些功能,而且對它們進行個人化化可能不符合道德,因為個人化會傳播概括性和偏見。 例如，工程師徵才公告若刻意不向年長或特定性別的對象顯示，即屬於這類偏見傳播。
* **區域資訊**:在世界許多地方,位置資訊(如郵政編碼、郵政編碼或鄰里名稱)可能與收入、種族和宗教高度相關。
* **使用者對公平性的看法**:即使您的應用程式正在做出合理的決策,也應考慮使用者感知應用程式中顯示的內容以似乎與具有歧視性的功能相關的方式更改的效果。
* **特徵中的意外偏差**:可以使用只影響總體子集的功能引入一些偏差類型。 如果以演算法產生特性，則需特別留意，例如，在使用影像分析擷取圖片中的項目時，以及使用文字分析探索文字中的實體時。 請留意您用來建立這些特性的服務有哪些特徵。

選擇要在內容和動作中傳送至個人化工具的特性時，請套用下列實務準則：

* 考量對某些應用程式使用特定特性是否合乎法律和道德，以及看似純正的特性是否可能遭他人轉化為您想要或應該避免的特性。
* 明確告知使用者他們所看到的選項會使用演算法和資料分析進行個人化。
* 問問自己:如果我使用這些信息來個人化他們的內容,我的用戶會關心並高興嗎? 我是否能坦然向使用者說明醒目提示或隱藏某些項目的決策是如何制定的？
* 使用行為資料，而不根據其他特性使用分類或分割資料。 過去，零售商基於歷史因素使用人口統計資訊 (在數位時代來臨前，人口統計屬性似乎較易於收集並作為行動依據)，但在已有與使用者的喜好和身分更為相關的實際互動、內容和歷史資料時，請自問人口統計資訊具有多少相關性。
* 考量如何防止特性遭到惡意使用者冒用，因為若遭到大量利用，可能導致個人化工具以錯誤的方式進行訓練，而刻意干擾、羞辱及遭擾特定類別的使用者。
* 在情況允許時，請將您的應用程式設計為允許使用者選擇加入或退出特定個人特性的使用。 這些特性可進行分組，例如「位置資訊」、「裝置資訊」、「過去購買記錄」等。


## <a name="computing-rewards-for-personalizer"></a>計算個人化工具的回報

個人化工具會持續改良根據您的應用程式商務邏輯所提供的回報分數來選擇要回報哪個動作的機制。

建置完善的回報分數將作為商務目標的短期 Proxy，並繫結至組織的任務。

例如，回報點擊率將使個人化工具服務積極追求點擊率，而犧牲掉其他一切，即使點擊的項目與正題或商務成果無關，仍是如此。

在相對的範例中，新聞網站可能會想要設定比點擊率更有意義的相關回報，例如「使用者是否真正花時間閱讀內容？」 「他們是否點擊了相關文章或參考資料？」。 透過個人化工具，計量將可輕易地密切繫結至回報。 但請留意不要將短期的使用者參與與實際成果混淆。

### <a name="unintended-consequences-from-reward-scores"></a>回報分數的非預期結果
雖然回報分數可依據良好立意而建置，但仍可能在個人化工具對內容的排名方面產生非預期的後果或結果。

請參考下列範例：

* 依據觀看影片的長度百分比來回報影片內容的個人化，可能會使短片獲得較高排名。
* 直接回報社交媒體分享，而不進行其分享方式或內容本身的情感分析，可能會導致冒犯性、未經審核或煽動性的內容獲得較高排名，這樣雖然可帶動大量「參與」，但卻無法增加實質價值。
* 回報使用者未預期要變更的使用者介面元素的動作，可能會干擾使用者介面的可用性和可預測性，因為按鈕未經警告即突然變更位置或用途，而使得特定使用者群組難以維持生產力。

請實作下列最佳做法：

* 使用不同的回報方法對您的系統執行離線實驗，以了解相關影響和副作用。
* 評估您的回報函式，並自問極度缺乏相關知識的使用者可能如何加以曲解，而產生不當的結果。


## <a name="responsible-design-considerations"></a>負責的設計考量

在設計負責的 AI 實作時，需考量以下幾方面。 參考 [The Future Computed](https://news.microsoft.com/futurecomputed/) 以深入了解此架構。

![The Future Computed 中介紹的 AI 價值](media/ethics-and-responsible-use/ai-values-future-computed.png)

### <a name="accountability"></a>責任
*AI 系統的設計和部署人員必須對其系統的運作方式負責*。

* 建立個人化工具實作方式的內部指導方針文件，並製成文件，將其傳達給您的小組成員、高層主管和供應商。
* 定期檢討回報分數的計算方式、執行離線評估以確認哪些特性會影響個人化工具，並以其結果消除不當和非必要的特性。
* 向您的使用者明確闡述個人化工具的使用方式、用途及其使用的資料。
* 封存個人化工具賴以重現結果的資訊和資產，例如模型、學習原則和其他資料。

### <a name="transparency"></a>透明度
*AI 系統必須是可理解的*。 透過個人化工具：

* *為使用者提供有關於內容如何個人化的資訊。* 例如，您可以向使用者顯示標示為 `Why These Suggestions?` 的按鈕，說明使用者和動作有哪些最高排名的特性會影響個人化工具的結果。
* 確定您的使用規定提及您將使用使用者及其行為的相關資訊將體驗個人化。

### <a name="fairness"></a>公平性
AI 系統應將所有人一視同仁**。

* 請勿將個人化工具用於結果屬長期、必然性或涉及實質損害的使用案例。
* 請勿使用不適合用於內容個人化或可能助長不當偏見傳播的特性。 例如，財務情況類似的每一個人，在金融產品方面應獲得相同的個人化建議。
* 了解特性中可能來自於編輯器、演算法工具或使用者本身的偏見。

### <a name="reliability-and-safety"></a>可靠性和安全性
*AI 系統應安全可靠地執行*。 針對個人化工具：

* *請勿對個人化工具提供不應選擇的動作*。 例如，為匿名或未成年使用者提供建議時，應從個人化的動作中篩選掉不適當的電影。
* *將個人化工具模型視同企業資產進行管理*。  請考量要在個人化工具迴圈以外儲存和備份模型和學習原則的頻率，或將其視為重要的企業資產。 能夠重現過去的結果，才能進行自我稽核及評估改進幅度。
* *提供適當通道以取得使用者直接的意見反應*。 除了編碼安全性檢查以外，若要確保所有受眾都只會看到適當的內容，請為使用者提供意見反應機制，用以回報非預期或造成干擾的內容。 特別是，如果您的內容來自使用者或第三方，請考慮使用 Microsoft Content Moderator 或其他工具來檢閱和驗證內容。
* *執行頻繁的離線評估*。 這有助於您監視趨勢並確知有效性。
* *建立偵測及因應惡意操作的程序*。 有些執行者會利用機器學習和 AI 系統的能力深入了解其環境，而操控出有利於其目標的結果。 如果您使用的個人化工具有可能影響重要選擇，請務必以適當機制偵測並緩解這些類別的攻擊，包括在適當的情況下進行人工審核。

### <a name="security-and-privacy"></a>安全性和隱私權
*AI 系統應安全且尊重隱私權*。 使用個人化工具時：

* 依照您當地和業界的法規，*事先告知使用者所將收集的資料及其使用方式，並取得其事先的同意*。
* *提供保護隱私權的使用者控制項。* 對於儲存個人資訊的應用程式，請考慮針對如下的功能提供容易尋找的按鈕：
   * `Show me all you know about me`
   * `Forget my last interaction`
   * `Delete all you know about me`

在某些情況下，法規會有此類要求。 請權衡是否要定期重新訓練模型，使其不含已刪除資料的追蹤。

### <a name="inclusiveness"></a>包容性
*因應各種人的需求和體驗*。
* *為具備協助工具功能的介面提供個人化體驗。* 適當個人化所產生的效益 (經套用可減少互動中的人力需求、移動及不必要的重複)，對於殘疾人士可能特別有幫助。
* *依據內容調整應用程式行為*。 您可以使用個人化工具來區分聊天機器人中的意圖，例如，正確的解譯方式可需視上下文而定，並非一體適用。


## <a name="proactive-readiness-for-increased-data-protection-and-governance"></a>主動整備以提升對資料的保護和控管

要預測法規內容的特定變更並不容易，但一般而言，最好將標準提高到最低法規架構之上，以確保在使用個人資料時能夠尊重隱私權，並提供演算法決策制定的透明度和選擇。


* 考慮事先進行規劃，以因應對個人收集的資料可能有新的限制，且需要說明該資料如何用來進行決策的情況。
* 考慮進行額外的整備，以因應使用者中可能包含邊緣弱勢人口、兒童、經濟弱勢使用者，或易受演算法操控影響之使用者的情況。
* 考量以受眾為目標和影響受眾的資料收集程式與演算法執行後所造成的不滿程度，並考量如何避免已確知的策略錯誤。


## <a name="proactive-assessments-during-your-project-lifecycle"></a>專案生命週期內的主動評量

考慮為小組成員、使用者和企業擁有者建立適當方法，用以回報與負責的使用有關的問題，以及建立為其解決方法設定優先順序並防止報復的程序。

任何人在考量使用任何技術的副作用時，都會受限於個人的觀點和生命經驗。 讓更多元的意見心聲進入您的小組、使用者或諮詢委員會，可鼓勵他們力陳己見。 請考慮建立訓練與學習材料，以進一步拓展小組成員在這個領域的知識，並提升討論複雜和敏感主題的能力。

請考慮像對待應用程式生命週期中的其他橫切任務一樣對待有關負責任使用的任務,例如與使用者體驗、安全性或 DevOps 相關的任務。 這些工作及其需求必須事先考量。 在整個應用程式生命週期內都必須討論並驗證負責的使用。

## <a name="questions-and-feedback"></a>問題與意見反應

Microsoft 會持續將心力投注於工具和文件，以協助您善盡相關責任。 如果您認為有其他工具、產品功能和文件可協助您遵循這些使用個人化工具的指導方針，請[提供意見反應給 Microsoft](mailto:cogsvcs-RL-feedback@microsoft.com?subject%3DPersonalizer%20Responsible%20Use%20Feedback&body%3D%5BPlease%20share%20any%20question%2C%20idea%20or%20concern%5D)。

## <a name="recommended-reading"></a>建議閱讀資料

* 請參閱 Microsoft 在 2018 年 1 月出版的《[未來計算](https://news.microsoft.com/futurecomputed/)》一書中發布的關於負責任地開發 AI 的六項原則
* [Who Owns the Future?](https://www.goodreads.com/book/show/15802693-who-owns-the-future) (作者：Jaron Lanier)。
* [Weapons of Math Destruction](https://www.goodreads.com/book/show/28186015-weapons-of-math-destruction) (作者：Cathy O'Neil)
* [Ethics and Data Science](https://www.oreilly.com/library/view/ethics-and-data/9781492043898/) (作者：DJ Patil、Hilary Mason、Mike Loukides)。
* [ACM 道德法規](https://www.acm.org/code-of-ethics)
* [基因資訊平等法 - GINA](https://en.wikipedia.org/wiki/Genetic_Information_Nondiscrimination_Act)
* [FATML 負責任的演算法原則](https://www.fatml.org/resources/principles-for-accountable-algorithms)


## <a name="next-steps"></a>後續步驟

[特點:操作和上下文](concepts-features.md)。
